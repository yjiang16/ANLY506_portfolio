---
title: "week10_Practice_Cluster"
author: "yjiang"
date: "7/11/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r }
# Install factoextra
installed.packages("factoextra")

#load the libraries
library(tidyverse)
library(cluster)
library(factoextra)

# Load data frame
df <- USArrests

# Remove the missing values
df <- na.omit(df)

# Scale/Standardizethe data
df <- scale(df)
head(df)


```
```{r}
# Create a distance matrix 
distance <- get_dist(df)
fviz_dist(distance, gradient = list(low = "#FC4E07", mid = "white", high = "#00AFBB"))

# Compute K-means cluster
k2 <- kmeans(df, centers = 3, nstart = 30)
str(k2)

# Create cluster diagram
fviz_cluster(k2, data = df)   # seems three variance

      #or similar function
df %>%
  as_tibble() %>%
  mutate(cluster = k2$cluster,
         state = row.names(USArrests)) %>%
  ggplot(aes(UrbanPop, Murder, color = factor(cluster), label = state)) +
  geom_text()


# Testing different data
## Create the dataset
k3 <- kmeans(df, centers = 3, nstart = 35)
k4 <- kmeans(df, centers = 4, nstart = 35)
k5 <- kmeans(df, centers = 5, nstart = 35)

## Create the diagrams to compare
p1 <- fviz_cluster(k2, geom = "point", data = df) + ggtitle("k = 2")
p2 <- fviz_cluster(k3, geom = "point",  data = df) + ggtitle("k = 3")
p3 <- fviz_cluster(k4, geom = "point",  data = df) + ggtitle("k = 4")
p4 <- fviz_cluster(k5, geom = "point",  data = df) + ggtitle("k = 5")

# To arrange the diagrams
library(gridExtra)
grid.arrange(p1, p2, p3, p4, nrow = 2)

```

```{r}
# Elbow Method
## det the seed
set.seed(222)

## Compute total whith cluster sum of square
wss <- function(k) {
  kmeans(df, k, nstart = 10 )$tot.withinss}

## Cumpute the plot
k.values <- 1:15

## Dry the diagram
wss_values <- map_dbl(k.values, wss)

plot(k.values, wss_values,
       type="b", pch = 19, frame = FALSE, 
       xlab="Number of clusters K",
       ylab="Total within-clusters sum of squares")

##
set.seed(222)

fviz_nbclust(df, kmeans, method = "wss")


```
```{r}
# Average silhouette method
## Compute average silhouette for k clusters
avg_sil <- function(k) {
  km.res <- kmeans(df, centers = k, nstart = 30)
  ss <- silhouette(km.res$cluster, dist(df))
  mean(ss[, 3])}

## Compute and plot
k.values <- 2:20

## Draw the clusters
avg_sil_values <- map_dbl(k.values, avg_sil)

plot(k.values, avg_sil_values,
       type = "b", pch = 9, frame = FALSE, 
       xlab = "Number of clusters K",
       ylab = "Average Silhouettes")

# Average silhoutte method
fviz_nbclust(df, kmeans, method = "silhouette")

```
```{r}
# Gap Statistic method 
## compute gap statistic
set.seed(222)
gap_stat <- clusGap(df, FUN = kmeans, nstart = 25,
                    K.max = 10, B = 50)
## Print
print(gap_stat, method = "firstmax")

# Visualize the results
fviz_gap_stat(gap_stat)

```
```{r}
# Compute k-means clustering with k = 3
set.seed(222)
final <- kmeans(df, 3, nstart = 20)
print(final)

## visualize the results
fviz_cluster(final, data = df)

# Extract the cluster 
USArrests %>%
  mutate(Cluster = final$cluster) %>%
  group_by(Cluster) %>%
  summarise_all("mean")

```
```{r}
# Hierarchical Cluster Analysis
## Load the library
installed.packages("dendextend")
library(dendextend)

# Agglomerative hierarchical clustering
## Dissimilarity matrix
d <- dist(df, method = "euclidean")

## Hierarchical clustering using Complete Linkage
hc1 <- hclust(d, method = "complete" )

## Plot
plot(hc1, cex = 0.6, hang = -5)

# Alternate
## Compute with agnes
hc2 <- agnes(df, method = "complete")

## Agglomerative coefficient
hc2$ac

# strong cluster strcture
## methods to assess
m <- c( "average", "single", "complete", "ward")
names(m) <- c( "average", "single", "complete", "ward")

## Compute coefficient
ac <- function(x) {
  agnes(df, method = x)$ac}

map_dbl(m, ac)

## Print the result
hc3 <- agnes(df, method = "ward")
pltree(hc3, cex = 0.6, hang = -3, main = "Dendrogram")

# Divisive Hierarchical
## compute divisive hierarchical clustering
hc4 <- diana(df)

## Divise coefficient; amount of clustering structure found
hc4$dc

## plot dendrogram
pltree(hc4, cex = 0.6, hang = -3, main = "Dendrogram")

# Try to cut the dendrogram
## Use hc5 as an example
hc5 <- hclust(d, method = "ward.D2" )

## Cut tree into 5 groups
sub_grp <- cutree(hc5, k = 5)

## Number of members in each cluster
table(sub_grp)

## Print the result
plot(hc5, cex = 0.6)
rect.hclust(hc5, k = 5, border = 2:5)


```
```{r}
# Other diagrams
## Visualize a scatter plot
fviz_cluster(list(data = df, cluster = sub_grp))

##Cut agnes() tree into 5 groups
hc_a <- agnes(df, method = "ward")
cutree(as.hclust(hc_a), k = 5)

## Cut diana() tree into 5 groups
hc_d <- diana(df)
cutree(as.hclust(hc_d), k = 5)

```
```{r}
# Compare two dendrograms
## Compute distance matrix
res.dist <- dist(df, method = "euclidean")

## Compute 2 hierarchical clusterings
hc1 <- hclust(res.dist, method = "complete")
hc2 <- hclust(res.dist, method = "ward.D2")

## Createthe two dendrograms
dd1 <- as.dendrogram (hc1)
dd2 <- as.dendrogram (hc2)

## Print the result
tanglegram(dd1, dd2)

# Change color
dend_list <- dendlist(dd1, dd2)

## print
tanglegram(dd1, dd2,
  highlight_distinct_edges = FALSE, # Turn-off dashed lines
  common_subtrees_color_lines = FALSE, # Turn-off line colors
  common_subtrees_color_branches = TRUE, # Color common branches 
  main = paste("entanglement =", round(entanglement(dend_list), 2)))


```
```{r}
# K-Means clustering
## Set the data base
set.seed(1234)
x <- rnorm(12, mean = rep(1:3, each = 4), sd = 0.2)
y <- rnorm(12, mean = rep(c(1, 2, 1), each = 4), sd = 0.2)
dataFrame <-data.frame(x, y)

## Try to build heat maps
dataMatrix <- as.matrix(dataFrame)[sample(1:12), ]
kmeansObj <- kmeans(dataMatrix, centers = 3)

## Make the image
par(mfrow = c(1, 2))
image(t(dataMatrix)[, nrow(dataMatrix):1], yaxt = "n", main = "Original Data")
image(t(dataMatrix)[, order(kmeansObj$cluster)], yaxt = "n", main = "Clustered Data")

```









